# AC215 - Milestone2 - SalesMate

**Team Members**
Rafael Hoffmann Fallgatter, Rajiv Swamy and Vincent Huang

**Group Name**
SalesMate

**Project**
The objective of this project is to develop a simulation of a sales chat where the AI plays the role of a client and the user must make a sale as a means of practice sales skills. More specifically, it will simulate a situation where the user is offering financial services. This was chosen since we have a dataset of past calls in this situation. 
The app will be a chat interface where the AI answers using a model fine-tuned in real sales conversations. Moreover, the app will also have a copilot, which gives recommendation to the seller of how to answer to technical questions by having access to a knowledge base of pdfs of finance documents using RAG. 

## Milestone2 ##

In this milestone, we have developed a first version of all the main components of the application, but not yet cconnecting them in single a application. The next step is to make those connections and improve performance of each component. 

### Data ###
We got access to 1235 sales calls, which we used to finetune the model. Those calls were made by a Brazilian company and therefore were all in Portuguese. Therefore the first step of the data processing was 

**Data Pipeline**
1. The code receives a csv with the transcripts of the calls, were each row is an entire call. It then uses Google Translate to translate the transcripts from portuguese to english keeping the same data format.

2. Since at this stage we are fine-tuning Gemini, we must put the data in the format it expects, that is, a jsol, were each row is a question and an answer, instead of an entire call. After this procedure, we have a jsol with over 10k rows. This is devidided into a train and a test set, were the test set has 250 cases, and the test set has the rest. 

See the data-translation container for more details of the translation dataprocessing pipeline.

**Data Versioning**

We are in the process of transitioning our cloud provider from GCP to AWS [we are still waiting for AWS credits for this as we are stuck on the free tier]. We aim to utilize Amazon Delta Lake for a naitively versioned data store as per the recommendation of our TF Mentor. Otherwise we will utilize an S3 bucket with DVC. 

### User interface ###

The frontend will look as in the following image, were the user has access to the chat with the AI, as well as the copilot.

![Mock](images/mock.jpg)

For this milestone, we are simulating the chat and the copilot separately in the local CLIs. 

**Chat**

The user interacts with the AI via a chat, as in a real sales conversation. The chat code makes an API call to a container in an EC2 in AWS that has pre-trained mini-llama model and generates the response. In the future, the response will be generated by a pre-trained model. 

**Copilot**

The goal of this module is to help the user by getting information from technical pdf documents using RAG. The process is as follows:

1. Add pdfs to local folder and generate a vector database

2. Receive the questions generated by the AI-seller and use Gemini to determine whether it´s necessary to search for information in the knowledge base. 

3. If it is necessary to search for information, it summarizes the questions, vectorizes it and searches for information in the database

4. Answers question with this information

### Model fine-tuning ###

This is a container responsible for fine-tuning an LLM in past calls data generated by the data processing pipeline.

For this milestone, we only trained with a sample of 1k rows, but in the future we will train with entire data. Moreover, we used Vertex AI to fine-tune gemini, since we havent yet received the AWS credits. When we receive it, we will use hugging face to fine tune other models on an EC2 and serve the model using Sage Maker. 

We trained the gemini-1.5-flash-002, for 3 epochs and adapter size of 4. Following are some training metrics:

![Training Loss](images/loss.png)

![Preds](images/correct_preds.png)

## Running the code

Instructions for running each of the parts of the code is contained in the README inside the folder of each component in src
